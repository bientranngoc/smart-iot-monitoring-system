# Giáº£i quyáº¿t váº¥n Ä‘á» Duplicate Data trong MongoDB

## ğŸ” Váº¥n Ä‘á»

Báº¡n Ä‘Ã£ phÃ¡t hiá»‡n Ä‘Ãºng: **Dá»¯ liá»‡u bá»‹ trÃ¹ng láº·p nhiá»u** trong MongoDB readings collection.

### NguyÃªn nhÃ¢n:
1. **Kafka consumer offset `'earliest'`**: Má»—i láº§n restart consumer â†’ Ä‘á»c láº¡i Táº¤T Cáº¢ messages tá»« Ä‘áº§u Kafka topic
2. **KhÃ´ng cÃ³ unique constraint**: MongoDB cho phÃ©p insert cÃ¹ng reading nhiá»u láº§n
3. **Multiple consumers**: Náº¿u cháº¡y nhiá»u consumer instance â†’ duplicate processing

---

## âœ… CÃ¡c giáº£i phÃ¡p Ä‘Ã£ Ã¡p dá»¥ng

### 1. **ThÃªm Unique Compound Index trong MongoDB**

**File:** `monitoring/models.py` â†’ `ReadingClient._connect()`

```python
# Tá»± Ä‘á»™ng táº¡o index (device_id, timestamp) UNIQUE
self._collection.create_index(
    [("device_id", 1), ("timestamp", 1)],
    unique=True,
    background=True
)
```

**Káº¿t quáº£:**
- âœ… MongoDB Tá»° Äá»˜NG reject duplicate readings (cÃ¹ng device_id + timestamp)
- âœ… KhÃ´ng cáº§n logic phá»©c táº¡p trong code
- âœ… Insert trÃ¹ng â†’ PyMongoError â†’ log debug vÃ  tiáº¿p tá»¥c

### 2. **Äá»•i Kafka offset reset: `earliest` â†’ `latest`**

**Files:** `scripts/consumer.py`, `monitoring/tasks.py`

```python
'auto.offset.reset': 'latest'  # Chá»‰ Ä‘á»c message Má»šI tá»« lÃºc consumer start
```

**Káº¿t quáº£:**
- âœ… Restart consumer â†’ KHÃ”NG Ä‘á»c láº¡i messages cÅ©
- âœ… Chá»‰ process messages má»›i publish sau khi consumer start
- âš ï¸ LÆ°u Ã½: Náº¿u consumer down â†’ messages trong thá»i gian Ä‘Ã³ bá»‹ miss (trá»« khi Kafka retention cÃ²n)

### 3. **Cáº£i thiá»‡n Error Handling**

**File:** `monitoring/models.py` â†’ `insert_reading()`

```python
except PyMongoError as e:
    if 'duplicate key error' in str(e).lower():
        logging.debug("Duplicate reading ignored")  # DEBUG level, khÃ´ng spam logs
        return None
    logging.error(f"PyMongoError: {e}")  # ERROR level cho lá»—i tháº­t
```

**Káº¿t quáº£:**
- âœ… Duplicate â†’ log level DEBUG (khÃ´ng spam)
- âœ… Lá»—i khÃ¡c â†’ log level ERROR

---

## ğŸ› ï¸ Script lÃ m sáº¡ch duplicates hiá»‡n táº¡i

**File:** `scripts/clean_duplicates.py`

### Cháº¡y script:
```powershell
docker exec -it iot-app python scripts/clean_duplicates.py
```

### Script sáº½:
1. âœ… Äáº¿m tá»•ng readings vÃ  phÃ¢n bá»‘ theo device
2. âœ… TÃ¬m duplicates (cÃ¹ng device_id + timestamp)
3. âœ… XÃ³a duplicates (giá»¯ láº¡i 1 báº£n ghi Ä‘áº§u tiÃªn)
4. âœ… Táº¡o unique index
5. âœ… Hiá»ƒn thá»‹ káº¿t quáº£ sau khi clean

**Output máº«u:**
```
Current statistics:
  Total readings: 150

Finding duplicates...
  Found 50 groups of duplicates

Removing duplicates...
  âœ“ Removed 100 duplicates
  
Final statistics:
  Total readings: 50
  Removed: 100
```

---

## ğŸ¯ So sÃ¡nh cÃ¡c phÆ°Æ¡ng phÃ¡p

| PhÆ°Æ¡ng phÃ¡p | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khuyáº¿n nghá»‹ |
|-------------|---------|------------|-------------|
| **Unique Index** | â€¢ Äáº£m báº£o 100% khÃ´ng duplicate<br>â€¢ Tá»± Ä‘á»™ng, khÃ´ng cáº§n code<br>â€¢ Performance tá»‘t | â€¢ Cáº§n cleanup duplicates cÅ© trÆ°á»›c | âœ… **KHUYáº¾N NGHá»Š** |
| **Offset 'latest'** | â€¢ Giáº£m duplicate khi restart | â€¢ Miss messages náº¿u consumer down<br>â€¢ KhÃ´ng ngÄƒn duplicate 100% | âš ï¸ Káº¿t há»£p vá»›i index |
| **Idempotent insert** | â€¢ Application-level control | â€¢ Cáº§n check exist trÆ°á»›c insert<br>â€¢ Cháº­m hÆ¡n | âŒ KhÃ´ng cáº§n náº¿u cÃ³ index |
| **Message deduplication** | â€¢ NgÄƒn duplicate á»Ÿ source | â€¢ Phá»©c táº¡p<br>â€¢ Cáº§n thÃªm cache/storage | âŒ Overkill |

---

## ğŸ“Š Kiáº¿n trÃºc Data Storage

### Táº¡i sao tÃ¡ch MySQL vÃ  MongoDB?

| Database | Data Type | Äáº·c Ä‘iá»ƒm | Use Case |
|----------|-----------|----------|----------|
| **MySQL** | Metadata | â€¢ Structured<br>â€¢ ACID transactions<br>â€¢ Relations (User â†’ Device) | â€¢ Quáº£n lÃ½ users<br>â€¢ Quáº£n lÃ½ devices<br>â€¢ Authentication<br>â€¢ Device ownership |
| **MongoDB** | Time-series | â€¢ Schema-flexible<br>â€¢ High write throughput<br>â€¢ Horizontal scaling | â€¢ Sensor readings<br>â€¢ Logs<br>â€¢ Analytics<br>â€¢ Historical data |

### LÃ m sao trÃ¡nh duplicate?

#### MySQL (Users, Devices):
```python
User.objects.get_or_create(username='default_user')  # Django ORM handle duplicate
Device.objects.get_or_create(name='Device 1')        # Database unique constraint
```
âœ… **KhÃ´ng duplicate** - Django ORM + DB constraints

#### MongoDB (Readings):
```python
# TrÆ°á»›c khi fix:
collection.insert_one(doc)  # CÃ³ thá»ƒ duplicate âŒ

# Sau khi fix (vá»›i unique index):
collection.insert_one(doc)  # Tá»± Ä‘á»™ng reject duplicate âœ…
```

---

## ğŸš€ CÃ¡c bÆ°á»›c triá»ƒn khai

### BÆ°á»›c 1: Clean duplicates hiá»‡n táº¡i
```powershell
docker exec -it iot-app python scripts/clean_duplicates.py
```

### BÆ°á»›c 2: Restart consumer Ä‘á»ƒ Ã¡p dá»¥ng offset má»›i
```powershell
# Dá»«ng consumer cÅ© (Ctrl+C)
docker exec -it iot-app python scripts/consumer.py
```

### BÆ°á»›c 3: Test vá»›i data má»›i
```powershell
# XÃ³a topic Ä‘á»ƒ test clean
docker exec -it iot-kafka kafka-topics --bootstrap-server localhost:9092 --delete --topic raw-data
docker exec -it iot-kafka kafka-topics --bootstrap-server localhost:9092 --create --topic raw-data --partitions 1 --replication-factor 1

# Publish data má»›i
docker exec -it iot-app python scripts/publish.py
```

### BÆ°á»›c 4: Verify khÃ´ng cÃ³ duplicate
```powershell
docker exec -it iot-mongodb mongosh -u root -p root --eval "
use iot;
db.readings.count();
db.readings.aggregate([
  {\$group: {
    _id: {\$concat: [{\$toString: '\$device_id'}, '-', {\$toString: '\$timestamp'}]},
    count: {\$sum: 1}
  }},
  {\$match: {count: {\$gt: 1}}}
]);
"
```

**Expected:** KhÃ´ng cÃ³ duplicate!

---

## ğŸ¯ Best Practices Ä‘Ã£ Ã¡p dá»¥ng

### 1. **Idempotency**
- MongoDB unique index â†’ duplicate insert = no-op
- Kafka offset commit â†’ message chá»‰ process 1 láº§n (náº¿u consumer stable)

### 2. **Data Integrity**
- MySQL foreign keys (User â†’ Device)
- MongoDB unique compound index (device_id, timestamp)

### 3. **Error Handling**
- Duplicate â†’ DEBUG log (khÃ´ng spam)
- Real errors â†’ ERROR log
- Graceful degradation

### 4. **Monitoring**
- Log inserted_id Ä‘á»ƒ track success
- Count by device Ä‘á»ƒ detect anomalies
- Test scripts Ä‘á»ƒ verify integrity

---

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i

### TrÆ°á»›c khi fix:
```javascript
db.readings.count()  // 150 (cÃ³ 100 duplicates)
```

### Sau khi fix:
```javascript
db.readings.count()  // 50 (unique readings only)

// Try insert duplicate manually:
db.readings.insertOne({device_id: 1, temperature: 25, humidity: 60, timestamp: ISODate("2025-10-20T04:00:00Z")})
db.readings.insertOne({device_id: 1, temperature: 25, humidity: 60, timestamp: ISODate("2025-10-20T04:00:00Z")})
// â†’ Error: E11000 duplicate key error âœ…
```

---

## ğŸ” Troubleshooting

### Váº«n tháº¥y duplicates sau khi fix?

1. **Check index Ä‘Ã£ táº¡o chÆ°a:**
```javascript
db.readings.getIndexes()
// Pháº£i cÃ³: { device_id: 1, timestamp: 1 } unique: true
```

2. **Check offset Ä‘Ã£ Ä‘á»•i chÆ°a:**
```python
# Trong consumer.py hoáº·c tasks.py
'auto.offset.reset': 'latest'  # Pháº£i lÃ  'latest'
```

3. **Check cÃ³ cháº¡y nhiá»u consumer khÃ´ng:**
```powershell
docker ps | findstr consumer
# Chá»‰ nÃªn cÃ³ 1 consumer cho 1 group_id
```

4. **Reset Kafka consumer group náº¿u cáº§n:**
```powershell
docker exec -it iot-kafka kafka-consumer-groups --bootstrap-server localhost:9092 --group iot-group --reset-offsets --to-latest --topic raw-data --execute
```

---

## âœ… Checklist

- [ ] Cháº¡y `clean_duplicates.py` Ä‘á»ƒ xÃ³a duplicates cÅ©
- [ ] Verify unique index Ä‘Ã£ táº¡o trong MongoDB
- [ ] Äá»•i `auto.offset.reset` thÃ nh `'latest'`
- [ ] Restart consumer vá»›i config má»›i
- [ ] Test publish â†’ khÃ´ng cÃ³ duplicate
- [ ] Monitor logs khÃ´ng tháº¥y spam "duplicate key error"

---

Há»‡ thá»‘ng giá» Ä‘Ã£ cÃ³ **deduplication tá»± Ä‘á»™ng**! ğŸ‰
